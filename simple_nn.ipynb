{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlMKkWYdqH7wSUberjqC9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anirudh-S/pytorch_initial/blob/main/simple_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lzKl0b-we4m-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, in_feat=4,h1=8,h2=9,o_f=3):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(in_feat,h1)\n",
        "    self.fc2=nn.Linear(h1,h2)\n",
        "    self.out=nn.Linear(h2,o_f)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=F.relu(self.fc2(x))\n",
        "    x=self.out(x)\n",
        "\n",
        "    return x\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Mj4Zb3p5fn0j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(41)\n",
        "model = Model()\n"
      ],
      "metadata": {
        "id": "wGVntTKchVWd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Ov2nRpvehreW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df = pd.read_csv(\"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv\")"
      ],
      "metadata": {
        "id": "6h6DAKcxiAm3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "on7VVhMBicDB",
        "outputId": "4a6e9645-5c24-43c4-fa3d-cdb5e491612a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width    species\n",
              "145           6.7          3.0           5.2          2.3  virginica\n",
              "146           6.3          2.5           5.0          1.9  virginica\n",
              "147           6.5          3.0           5.2          2.0  virginica\n",
              "148           6.2          3.4           5.4          2.3  virginica\n",
              "149           5.9          3.0           5.1          1.8  virginica"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32b4384b-847f-4bc4-bd6a-1fc08d90713d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32b4384b-847f-4bc4-bd6a-1fc08d90713d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-32b4384b-847f-4bc4-bd6a-1fc08d90713d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-32b4384b-847f-4bc4-bd6a-1fc08d90713d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4c65645d-79bb-4428-96f3-68315dfc6d22\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c65645d-79bb-4428-96f3-68315dfc6d22')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4c65645d-79bb-4428-96f3-68315dfc6d22 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"my_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30331501776206193,\n        \"min\": 5.9,\n        \"max\": 6.7,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          6.3,\n          5.9,\n          6.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31937438845342625,\n        \"min\": 2.5,\n        \"max\": 3.4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.0,\n          2.5,\n          3.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14832396974191348,\n        \"min\": 5.0,\n        \"max\": 5.4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5.0,\n          5.1,\n          5.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23021728866442667,\n        \"min\": 1.8,\n        \"max\": 2.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.9,\n          1.8,\n          2.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"species\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"virginica\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_df['species'] = my_df['species'].replace('setosa',0.0)\n",
        "my_df['species'] = my_df['species'].replace('virginica',2.0)\n",
        "my_df['species'] = my_df['species'].replace('versicolor',1.0)\n",
        "my_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "FPlE1zeRifiX",
        "outputId": "1673231c-9970-43bb-f398-65097b182e7f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a3d6d4a2abc0>:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  my_df['species'] = my_df['species'].replace('versicolor',1.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width  species\n",
              "0             5.1          3.5           1.4          0.2      0.0\n",
              "1             4.9          3.0           1.4          0.2      0.0\n",
              "2             4.7          3.2           1.3          0.2      0.0\n",
              "3             4.6          3.1           1.5          0.2      0.0\n",
              "4             5.0          3.6           1.4          0.2      0.0\n",
              "..            ...          ...           ...          ...      ...\n",
              "145           6.7          3.0           5.2          2.3      2.0\n",
              "146           6.3          2.5           5.0          1.9      2.0\n",
              "147           6.5          3.0           5.2          2.0      2.0\n",
              "148           6.2          3.4           5.4          2.3      2.0\n",
              "149           5.9          3.0           5.1          1.8      2.0\n",
              "\n",
              "[150 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ce22c05-d439-4c66-9ede-845956ead4b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ce22c05-d439-4c66-9ede-845956ead4b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ce22c05-d439-4c66-9ede-845956ead4b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ce22c05-d439-4c66-9ede-845956ead4b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-15e337df-a814-4dd3-908b-f98542f1e029\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-15e337df-a814-4dd3-908b-f98542f1e029')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-15e337df-a814-4dd3-908b-f98542f1e029 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_30760122-ac11-40f5-a564-7aef9b2119be\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('my_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_30760122-ac11-40f5-a564-7aef9b2119be button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('my_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "my_df",
              "summary": "{\n  \"name\": \"my_df\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8280661279778629,\n        \"min\": 4.3,\n        \"max\": 7.9,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          6.2,\n          4.5,\n          5.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4335943113621737,\n        \"min\": 2.0,\n        \"max\": 4.4,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          2.3,\n          4.0,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7644204199522617,\n        \"min\": 1.0,\n        \"max\": 6.9,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          6.7,\n          3.8,\n          3.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7631607417008414,\n        \"min\": 0.1,\n        \"max\": 2.5,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.2,\n          1.2,\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"species\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8192319205190405,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=my_df.drop('species',axis=1)\n",
        "y=my_df['species']\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "fL7qXTaljNrC",
        "outputId": "80509a08-d35f-497a-b921-04b4dd7570be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.0\n",
              "1      0.0\n",
              "2      0.0\n",
              "3      0.0\n",
              "4      0.0\n",
              "      ... \n",
              "145    2.0\n",
              "146    2.0\n",
              "147    2.0\n",
              "148    2.0\n",
              "149    2.0\n",
              "Name: species, Length: 150, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "mJApczxXkBVb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=41)"
      ],
      "metadata": {
        "id": "ouMORzgQkBwE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor(x_train.to_numpy())\n",
        "x_test = torch.FloatTensor(x_test.to_numpy())"
      ],
      "metadata": {
        "id": "wzckQvcazgoY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = torch.LongTensor(y_train.to_numpy())\n",
        "y_test = torch.LongTensor(y_test.to_numpy())"
      ],
      "metadata": {
        "id": "xAMYWLc90sU4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "Buc2ZmNI3m2o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500\n",
        "losses = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  y_pred = model.forward(x_train)\n",
        "  loss = criterion(y_pred,y_train)\n",
        "  losses.append(loss.detach().numpy())\n",
        "  print(f\"epoch {i} and loss is {loss}\")\n",
        "\n",
        "  if i%10==0:\n",
        "    print(f\"epoch {i} and loss is {loss}\")\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf2ED-LxLRM9",
        "outputId": "5c2e0d98-6a50-4b6e-e57a-59a3873e0055"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 and loss is 1.1251550912857056\n",
            "epoch 0 and loss is 1.1251550912857056\n",
            "epoch 1 and loss is 1.1095242500305176\n",
            "epoch 2 and loss is 1.0960756540298462\n",
            "epoch 3 and loss is 1.0845398902893066\n",
            "epoch 4 and loss is 1.073915719985962\n",
            "epoch 5 and loss is 1.0637413263320923\n",
            "epoch 6 and loss is 1.0538722276687622\n",
            "epoch 7 and loss is 1.0440350770950317\n",
            "epoch 8 and loss is 1.0337213277816772\n",
            "epoch 9 and loss is 1.0227365493774414\n",
            "epoch 10 and loss is 1.0096259117126465\n",
            "epoch 10 and loss is 1.0096259117126465\n",
            "epoch 11 and loss is 0.9954349398612976\n",
            "epoch 12 and loss is 0.9828447103500366\n",
            "epoch 13 and loss is 0.9664683938026428\n",
            "epoch 14 and loss is 0.9485272169113159\n",
            "epoch 15 and loss is 0.9305612444877625\n",
            "epoch 16 and loss is 0.9112032651901245\n",
            "epoch 17 and loss is 0.8891754150390625\n",
            "epoch 18 and loss is 0.8652693629264832\n",
            "epoch 19 and loss is 0.8407701253890991\n",
            "epoch 20 and loss is 0.8157405853271484\n",
            "epoch 20 and loss is 0.8157405853271484\n",
            "epoch 21 and loss is 0.7899296283721924\n",
            "epoch 22 and loss is 0.7633073925971985\n",
            "epoch 23 and loss is 0.7376409769058228\n",
            "epoch 24 and loss is 0.7132112383842468\n",
            "epoch 25 and loss is 0.6902050375938416\n",
            "epoch 26 and loss is 0.6690837740898132\n",
            "epoch 27 and loss is 0.6484997272491455\n",
            "epoch 28 and loss is 0.627752423286438\n",
            "epoch 29 and loss is 0.6065686345100403\n",
            "epoch 30 and loss is 0.585706353187561\n",
            "epoch 30 and loss is 0.585706353187561\n",
            "epoch 31 and loss is 0.5654083490371704\n",
            "epoch 32 and loss is 0.5454426407814026\n",
            "epoch 33 and loss is 0.5255759954452515\n",
            "epoch 34 and loss is 0.5060237050056458\n",
            "epoch 35 and loss is 0.48700016736984253\n",
            "epoch 36 and loss is 0.4685356318950653\n",
            "epoch 37 and loss is 0.4503938555717468\n",
            "epoch 38 and loss is 0.4329933822154999\n",
            "epoch 39 and loss is 0.4162401556968689\n",
            "epoch 40 and loss is 0.3999636471271515\n",
            "epoch 40 and loss is 0.3999636471271515\n",
            "epoch 41 and loss is 0.38433876633644104\n",
            "epoch 42 and loss is 0.36935704946517944\n",
            "epoch 43 and loss is 0.3548978567123413\n",
            "epoch 44 and loss is 0.34101882576942444\n",
            "epoch 45 and loss is 0.32765448093414307\n",
            "epoch 46 and loss is 0.3147183954715729\n",
            "epoch 47 and loss is 0.30236876010894775\n",
            "epoch 48 and loss is 0.2903994023799896\n",
            "epoch 49 and loss is 0.27880606055259705\n",
            "epoch 50 and loss is 0.26768800616264343\n",
            "epoch 50 and loss is 0.26768800616264343\n",
            "epoch 51 and loss is 0.2568717300891876\n",
            "epoch 52 and loss is 0.24640172719955444\n",
            "epoch 53 and loss is 0.23634830117225647\n",
            "epoch 54 and loss is 0.22682783007621765\n",
            "epoch 55 and loss is 0.21810868382453918\n",
            "epoch 56 and loss is 0.2097075879573822\n",
            "epoch 57 and loss is 0.20173297822475433\n",
            "epoch 58 and loss is 0.1940319687128067\n",
            "epoch 59 and loss is 0.18659552931785583\n",
            "epoch 60 and loss is 0.17942361533641815\n",
            "epoch 60 and loss is 0.17942361533641815\n",
            "epoch 61 and loss is 0.17248447239398956\n",
            "epoch 62 and loss is 0.1657986044883728\n",
            "epoch 63 and loss is 0.15937189757823944\n",
            "epoch 64 and loss is 0.1532224863767624\n",
            "epoch 65 and loss is 0.1473274677991867\n",
            "epoch 66 and loss is 0.14168007671833038\n",
            "epoch 67 and loss is 0.13629387319087982\n",
            "epoch 68 and loss is 0.13111737370491028\n",
            "epoch 69 and loss is 0.12621845304965973\n",
            "epoch 70 and loss is 0.12151690572500229\n",
            "epoch 70 and loss is 0.12151690572500229\n",
            "epoch 71 and loss is 0.11706176400184631\n",
            "epoch 72 and loss is 0.1128450259566307\n",
            "epoch 73 and loss is 0.10882937908172607\n",
            "epoch 74 and loss is 0.10502595454454422\n",
            "epoch 75 and loss is 0.10142137110233307\n",
            "epoch 76 and loss is 0.09799394756555557\n",
            "epoch 77 and loss is 0.09475617110729218\n",
            "epoch 78 and loss is 0.09168296307325363\n",
            "epoch 79 and loss is 0.08876660466194153\n",
            "epoch 80 and loss is 0.0860118493437767\n",
            "epoch 80 and loss is 0.0860118493437767\n",
            "epoch 81 and loss is 0.08339236676692963\n",
            "epoch 82 and loss is 0.08091451227664948\n",
            "epoch 83 and loss is 0.07856667786836624\n",
            "epoch 84 and loss is 0.07633890211582184\n",
            "epoch 85 and loss is 0.07423163950443268\n",
            "epoch 86 and loss is 0.072228342294693\n",
            "epoch 87 and loss is 0.07033227384090424\n",
            "epoch 88 and loss is 0.0685340166091919\n",
            "epoch 89 and loss is 0.06682577729225159\n",
            "epoch 90 and loss is 0.06520850956439972\n",
            "epoch 90 and loss is 0.06520850956439972\n",
            "epoch 91 and loss is 0.06367207318544388\n",
            "epoch 92 and loss is 0.06222071126103401\n",
            "epoch 93 and loss is 0.06083771958947182\n",
            "epoch 94 and loss is 0.05952363461256027\n",
            "epoch 95 and loss is 0.05827471986413002\n",
            "epoch 96 and loss is 0.0570850744843483\n",
            "epoch 97 and loss is 0.05595705658197403\n",
            "epoch 98 and loss is 0.05487606301903725\n",
            "epoch 99 and loss is 0.0538492351770401\n",
            "epoch 100 and loss is 0.05286576226353645\n",
            "epoch 100 and loss is 0.05286576226353645\n",
            "epoch 101 and loss is 0.05192739516496658\n",
            "epoch 102 and loss is 0.051032885909080505\n",
            "epoch 103 and loss is 0.05017401650547981\n",
            "epoch 104 and loss is 0.04935476928949356\n",
            "epoch 105 and loss is 0.048566725105047226\n",
            "epoch 106 and loss is 0.047813013195991516\n",
            "epoch 107 and loss is 0.04708843678236008\n",
            "epoch 108 and loss is 0.04639375954866409\n",
            "epoch 109 and loss is 0.04572628065943718\n",
            "epoch 110 and loss is 0.04508381709456444\n",
            "epoch 110 and loss is 0.04508381709456444\n",
            "epoch 111 and loss is 0.044466257095336914\n",
            "epoch 112 and loss is 0.04387102276086807\n",
            "epoch 113 and loss is 0.04329812526702881\n",
            "epoch 114 and loss is 0.042744796723127365\n",
            "epoch 115 and loss is 0.042211469262838364\n",
            "epoch 116 and loss is 0.041695933789014816\n",
            "epoch 117 and loss is 0.04119817540049553\n",
            "epoch 118 and loss is 0.040716517716646194\n",
            "epoch 119 and loss is 0.040250614285469055\n",
            "epoch 120 and loss is 0.03979949280619621\n",
            "epoch 120 and loss is 0.03979949280619621\n",
            "epoch 121 and loss is 0.03936265408992767\n",
            "epoch 122 and loss is 0.038939882069826126\n",
            "epoch 123 and loss is 0.038530007004737854\n",
            "epoch 124 and loss is 0.038132235407829285\n",
            "epoch 125 and loss is 0.03774603456258774\n",
            "epoch 126 and loss is 0.03737092390656471\n",
            "epoch 127 and loss is 0.037006281316280365\n",
            "epoch 128 and loss is 0.03665173053741455\n",
            "epoch 129 and loss is 0.03630662336945534\n",
            "epoch 130 and loss is 0.035970740020275116\n",
            "epoch 130 and loss is 0.035970740020275116\n",
            "epoch 131 and loss is 0.035643287003040314\n",
            "epoch 132 and loss is 0.0353240892291069\n",
            "epoch 133 and loss is 0.035012051463127136\n",
            "epoch 134 and loss is 0.03470684215426445\n",
            "epoch 135 and loss is 0.03440602868795395\n",
            "epoch 136 and loss is 0.03410787135362625\n",
            "epoch 137 and loss is 0.03381018713116646\n",
            "epoch 138 and loss is 0.03351515531539917\n",
            "epoch 139 and loss is 0.033223651349544525\n",
            "epoch 140 and loss is 0.03293721750378609\n",
            "epoch 140 and loss is 0.03293721750378609\n",
            "epoch 141 and loss is 0.03265644982457161\n",
            "epoch 142 and loss is 0.03238225728273392\n",
            "epoch 143 and loss is 0.032115306705236435\n",
            "epoch 144 and loss is 0.031856048852205276\n",
            "epoch 145 and loss is 0.031604669988155365\n",
            "epoch 146 and loss is 0.03136082738637924\n",
            "epoch 147 and loss is 0.031123455613851547\n",
            "epoch 148 and loss is 0.03089258447289467\n",
            "epoch 149 and loss is 0.030659856274724007\n",
            "epoch 150 and loss is 0.03041938506066799\n",
            "epoch 150 and loss is 0.03041938506066799\n",
            "epoch 151 and loss is 0.03012103959918022\n",
            "epoch 152 and loss is 0.02955295704305172\n",
            "epoch 153 and loss is 0.0291855800896883\n",
            "epoch 154 and loss is 0.02884110063314438\n",
            "epoch 155 and loss is 0.028513574972748756\n",
            "epoch 156 and loss is 0.028195997700095177\n",
            "epoch 157 and loss is 0.027839388698339462\n",
            "epoch 158 and loss is 0.027499016374349594\n",
            "epoch 159 and loss is 0.027155935764312744\n",
            "epoch 160 and loss is 0.026795217767357826\n",
            "epoch 160 and loss is 0.026795217767357826\n",
            "epoch 161 and loss is 0.02644726075232029\n",
            "epoch 162 and loss is 0.02610182762145996\n",
            "epoch 163 and loss is 0.025743652135133743\n",
            "epoch 164 and loss is 0.025392575189471245\n",
            "epoch 165 and loss is 0.025051046162843704\n",
            "epoch 166 and loss is 0.02471129037439823\n",
            "epoch 167 and loss is 0.02437683194875717\n",
            "epoch 168 and loss is 0.02403920516371727\n",
            "epoch 169 and loss is 0.023708714172244072\n",
            "epoch 170 and loss is 0.02338254824280739\n",
            "epoch 170 and loss is 0.02338254824280739\n",
            "epoch 171 and loss is 0.02306351810693741\n",
            "epoch 172 and loss is 0.02275003306567669\n",
            "epoch 173 and loss is 0.022441893815994263\n",
            "epoch 174 and loss is 0.0221415925770998\n",
            "epoch 175 and loss is 0.021854085847735405\n",
            "epoch 176 and loss is 0.021620547398924828\n",
            "epoch 177 and loss is 0.021405763924121857\n",
            "epoch 178 and loss is 0.021193545311689377\n",
            "epoch 179 and loss is 0.020856210961937904\n",
            "epoch 180 and loss is 0.020531881600618362\n",
            "epoch 180 and loss is 0.020531881600618362\n",
            "epoch 181 and loss is 0.020349301397800446\n",
            "epoch 182 and loss is 0.020196562632918358\n",
            "epoch 183 and loss is 0.019939443096518517\n",
            "epoch 184 and loss is 0.019647328183054924\n",
            "epoch 185 and loss is 0.019448434934020042\n",
            "epoch 186 and loss is 0.019306479021906853\n",
            "epoch 187 and loss is 0.019112160429358482\n",
            "epoch 188 and loss is 0.01886124722659588\n",
            "epoch 189 and loss is 0.018647102639079094\n",
            "epoch 190 and loss is 0.018495969474315643\n",
            "epoch 190 and loss is 0.018495969474315643\n",
            "epoch 191 and loss is 0.018348194658756256\n",
            "epoch 192 and loss is 0.01815880462527275\n",
            "epoch 193 and loss is 0.01794644631445408\n",
            "epoch 194 and loss is 0.017760183662176132\n",
            "epoch 195 and loss is 0.017611173912882805\n",
            "epoch 196 and loss is 0.017473962157964706\n",
            "epoch 197 and loss is 0.0173227246850729\n",
            "epoch 198 and loss is 0.01715015061199665\n",
            "epoch 199 and loss is 0.01697133108973503\n",
            "epoch 200 and loss is 0.016802635043859482\n",
            "epoch 200 and loss is 0.016802635043859482\n",
            "epoch 201 and loss is 0.01665087416768074\n",
            "epoch 202 and loss is 0.016513044014573097\n",
            "epoch 203 and loss is 0.016383914276957512\n",
            "epoch 204 and loss is 0.016260571777820587\n",
            "epoch 205 and loss is 0.016141066327691078\n",
            "epoch 206 and loss is 0.016027530655264854\n",
            "epoch 207 and loss is 0.015911828726530075\n",
            "epoch 208 and loss is 0.015796594321727753\n",
            "epoch 209 and loss is 0.0156626608222723\n",
            "epoch 210 and loss is 0.015519778244197369\n",
            "epoch 210 and loss is 0.015519778244197369\n",
            "epoch 211 and loss is 0.01536103431135416\n",
            "epoch 212 and loss is 0.0152068380266428\n",
            "epoch 213 and loss is 0.015063784085214138\n",
            "epoch 214 and loss is 0.014936757273972034\n",
            "epoch 215 and loss is 0.01482374593615532\n",
            "epoch 216 and loss is 0.014721885323524475\n",
            "epoch 217 and loss is 0.01463139709085226\n",
            "epoch 218 and loss is 0.014556149952113628\n",
            "epoch 219 and loss is 0.014510747976601124\n",
            "epoch 220 and loss is 0.014496986754238605\n",
            "epoch 220 and loss is 0.014496986754238605\n",
            "epoch 221 and loss is 0.014538849703967571\n",
            "epoch 222 and loss is 0.014519542455673218\n",
            "epoch 223 and loss is 0.014420813880860806\n",
            "epoch 224 and loss is 0.014100625179708004\n",
            "epoch 225 and loss is 0.01381606049835682\n",
            "epoch 226 and loss is 0.013713065534830093\n",
            "epoch 227 and loss is 0.013758999295532703\n",
            "epoch 228 and loss is 0.013799070380628109\n",
            "epoch 229 and loss is 0.01366390660405159\n",
            "epoch 230 and loss is 0.013443661853671074\n",
            "epoch 230 and loss is 0.013443661853671074\n",
            "epoch 231 and loss is 0.013267917558550835\n",
            "epoch 232 and loss is 0.013224316760897636\n",
            "epoch 233 and loss is 0.013243179768323898\n",
            "epoch 234 and loss is 0.013196912594139576\n",
            "epoch 235 and loss is 0.013068835251033306\n",
            "epoch 236 and loss is 0.012898958288133144\n",
            "epoch 237 and loss is 0.012784821912646294\n",
            "epoch 238 and loss is 0.012742595747113228\n",
            "epoch 239 and loss is 0.012722793035209179\n",
            "epoch 240 and loss is 0.012677091173827648\n",
            "epoch 240 and loss is 0.012677091173827648\n",
            "epoch 241 and loss is 0.012573754414916039\n",
            "epoch 242 and loss is 0.012449578382074833\n",
            "epoch 243 and loss is 0.012336709536612034\n",
            "epoch 244 and loss is 0.012258818373084068\n",
            "epoch 245 and loss is 0.012209717184305191\n",
            "epoch 246 and loss is 0.012168782763183117\n",
            "epoch 247 and loss is 0.012121608480811119\n",
            "epoch 248 and loss is 0.01205301284790039\n",
            "epoch 249 and loss is 0.011971795000135899\n",
            "epoch 250 and loss is 0.011877342127263546\n",
            "epoch 250 and loss is 0.011877342127263546\n",
            "epoch 251 and loss is 0.011784343048930168\n",
            "epoch 252 and loss is 0.011696684174239635\n",
            "epoch 253 and loss is 0.011618112213909626\n",
            "epoch 254 and loss is 0.011547625064849854\n",
            "epoch 255 and loss is 0.011483519338071346\n",
            "epoch 256 and loss is 0.01142452284693718\n",
            "epoch 257 and loss is 0.011370818130671978\n",
            "epoch 258 and loss is 0.011326038278639317\n",
            "epoch 259 and loss is 0.011293569579720497\n",
            "epoch 260 and loss is 0.01128967385739088\n",
            "epoch 260 and loss is 0.01128967385739088\n",
            "epoch 261 and loss is 0.011316492222249508\n",
            "epoch 262 and loss is 0.011416422203183174\n",
            "epoch 263 and loss is 0.01151407789438963\n",
            "epoch 264 and loss is 0.011632201261818409\n",
            "epoch 265 and loss is 0.01144787110388279\n",
            "epoch 266 and loss is 0.011122316122055054\n",
            "epoch 267 and loss is 0.010767781175673008\n",
            "epoch 268 and loss is 0.010681051760911942\n",
            "epoch 269 and loss is 0.010816892609000206\n",
            "epoch 270 and loss is 0.010908301919698715\n",
            "epoch 270 and loss is 0.010908301919698715\n",
            "epoch 271 and loss is 0.010824841447174549\n",
            "epoch 272 and loss is 0.010557274334132671\n",
            "epoch 273 and loss is 0.010381813161075115\n",
            "epoch 274 and loss is 0.010392734780907631\n",
            "epoch 275 and loss is 0.010466526262462139\n",
            "epoch 276 and loss is 0.010453891940414906\n",
            "epoch 277 and loss is 0.010292736813426018\n",
            "epoch 278 and loss is 0.010132632218301296\n",
            "epoch 279 and loss is 0.010074884630739689\n",
            "epoch 280 and loss is 0.010098334401845932\n",
            "epoch 280 and loss is 0.010098334401845932\n",
            "epoch 281 and loss is 0.010107329115271568\n",
            "epoch 282 and loss is 0.010027349926531315\n",
            "epoch 283 and loss is 0.009905914776027203\n",
            "epoch 284 and loss is 0.009808910079300404\n",
            "epoch 285 and loss is 0.009773424826562405\n",
            "epoch 286 and loss is 0.009770489297807217\n",
            "epoch 287 and loss is 0.00974650401622057\n",
            "epoch 288 and loss is 0.009684771299362183\n",
            "epoch 289 and loss is 0.009594407863914967\n",
            "epoch 290 and loss is 0.009513933211565018\n",
            "epoch 290 and loss is 0.009513933211565018\n",
            "epoch 291 and loss is 0.009461955167353153\n",
            "epoch 292 and loss is 0.00943315401673317\n",
            "epoch 293 and loss is 0.009408985264599323\n",
            "epoch 294 and loss is 0.009370720013976097\n",
            "epoch 295 and loss is 0.009316685609519482\n",
            "epoch 296 and loss is 0.009249817579984665\n",
            "epoch 297 and loss is 0.009183491580188274\n",
            "epoch 298 and loss is 0.00912498403340578\n",
            "epoch 299 and loss is 0.009077099151909351\n",
            "epoch 300 and loss is 0.00903741829097271\n",
            "epoch 300 and loss is 0.00903741829097271\n",
            "epoch 301 and loss is 0.009001458063721657\n",
            "epoch 302 and loss is 0.008965852670371532\n",
            "epoch 303 and loss is 0.00892727356404066\n",
            "epoch 304 and loss is 0.00888667069375515\n",
            "epoch 305 and loss is 0.008842251263558865\n",
            "epoch 306 and loss is 0.008797334507107735\n",
            "epoch 307 and loss is 0.008750339038670063\n",
            "epoch 308 and loss is 0.008704674430191517\n",
            "epoch 309 and loss is 0.008658736944198608\n",
            "epoch 310 and loss is 0.008615588769316673\n",
            "epoch 310 and loss is 0.008615588769316673\n",
            "epoch 311 and loss is 0.008573613129556179\n",
            "epoch 312 and loss is 0.008536139503121376\n",
            "epoch 313 and loss is 0.008501777425408363\n",
            "epoch 314 and loss is 0.00847614649683237\n",
            "epoch 315 and loss is 0.008457424119114876\n",
            "epoch 316 and loss is 0.008457520976662636\n",
            "epoch 317 and loss is 0.00847009290009737\n",
            "epoch 318 and loss is 0.008519104681909084\n",
            "epoch 319 and loss is 0.008569617755711079\n",
            "epoch 320 and loss is 0.008653796277940273\n",
            "epoch 320 and loss is 0.008653796277940273\n",
            "epoch 321 and loss is 0.008647250011563301\n",
            "epoch 322 and loss is 0.00859013106673956\n",
            "epoch 323 and loss is 0.008364586159586906\n",
            "epoch 324 and loss is 0.008124363608658314\n",
            "epoch 325 and loss is 0.007956065237522125\n",
            "epoch 326 and loss is 0.007930757477879524\n",
            "epoch 327 and loss is 0.008002364076673985\n",
            "epoch 328 and loss is 0.008063680492341518\n",
            "epoch 329 and loss is 0.008056746795773506\n",
            "epoch 330 and loss is 0.007937375456094742\n",
            "epoch 330 and loss is 0.007937375456094742\n",
            "epoch 331 and loss is 0.007791442796587944\n",
            "epoch 332 and loss is 0.0076865036971867085\n",
            "epoch 333 and loss is 0.007661739364266396\n",
            "epoch 334 and loss is 0.007687770761549473\n",
            "epoch 335 and loss is 0.007703415118157864\n",
            "epoch 336 and loss is 0.007676864042878151\n",
            "epoch 337 and loss is 0.007595814298838377\n",
            "epoch 338 and loss is 0.007503347937017679\n",
            "epoch 339 and loss is 0.007432880811393261\n",
            "epoch 340 and loss is 0.007401105482131243\n",
            "epoch 340 and loss is 0.007401105482131243\n",
            "epoch 341 and loss is 0.007395818829536438\n",
            "epoch 342 and loss is 0.007390680257230997\n",
            "epoch 343 and loss is 0.007368984166532755\n",
            "epoch 344 and loss is 0.007321289274841547\n",
            "epoch 345 and loss is 0.007261219900101423\n",
            "epoch 346 and loss is 0.007200910709798336\n",
            "epoch 347 and loss is 0.007153371348977089\n",
            "epoch 348 and loss is 0.007121184840798378\n",
            "epoch 349 and loss is 0.0070995246060192585\n",
            "epoch 350 and loss is 0.007080807350575924\n",
            "epoch 350 and loss is 0.007080807350575924\n",
            "epoch 351 and loss is 0.007057527080178261\n",
            "epoch 352 and loss is 0.007027668412774801\n",
            "epoch 353 and loss is 0.006989629473537207\n",
            "epoch 354 and loss is 0.006947652902454138\n",
            "epoch 355 and loss is 0.0069037615321576595\n",
            "epoch 356 and loss is 0.006861694157123566\n",
            "epoch 357 and loss is 0.006822856608778238\n",
            "epoch 358 and loss is 0.006787811405956745\n",
            "epoch 359 and loss is 0.006756061688065529\n",
            "epoch 360 and loss is 0.006726674735546112\n",
            "epoch 360 and loss is 0.006726674735546112\n",
            "epoch 361 and loss is 0.006698842626065016\n",
            "epoch 362 and loss is 0.006671902257949114\n",
            "epoch 363 and loss is 0.006645753514021635\n",
            "epoch 364 and loss is 0.006620124913752079\n",
            "epoch 365 and loss is 0.006595908664166927\n",
            "epoch 366 and loss is 0.006572789046913385\n",
            "epoch 367 and loss is 0.006553041283041239\n",
            "epoch 368 and loss is 0.006535988766700029\n",
            "epoch 369 and loss is 0.006526059005409479\n",
            "epoch 370 and loss is 0.006521536968648434\n",
            "epoch 370 and loss is 0.006521536968648434\n",
            "epoch 371 and loss is 0.006531153339892626\n",
            "epoch 372 and loss is 0.006548414006829262\n",
            "epoch 373 and loss is 0.006589164026081562\n",
            "epoch 374 and loss is 0.006628569681197405\n",
            "epoch 375 and loss is 0.006687691900879145\n",
            "epoch 376 and loss is 0.0066968947649002075\n",
            "epoch 377 and loss is 0.006679559126496315\n",
            "epoch 378 and loss is 0.00655088247731328\n",
            "epoch 379 and loss is 0.006380841135978699\n",
            "epoch 380 and loss is 0.006200295872986317\n",
            "epoch 380 and loss is 0.006200295872986317\n",
            "epoch 381 and loss is 0.0060973153449594975\n",
            "epoch 382 and loss is 0.006091445684432983\n",
            "epoch 383 and loss is 0.00614320021122694\n",
            "epoch 384 and loss is 0.006193211767822504\n",
            "epoch 385 and loss is 0.006183216813951731\n",
            "epoch 386 and loss is 0.006117796991020441\n",
            "epoch 387 and loss is 0.006012261845171452\n",
            "epoch 388 and loss is 0.005923546850681305\n",
            "epoch 389 and loss is 0.005881726276129484\n",
            "epoch 390 and loss is 0.00588409136980772\n",
            "epoch 390 and loss is 0.00588409136980772\n",
            "epoch 391 and loss is 0.005901546683162451\n",
            "epoch 392 and loss is 0.005900225136429071\n",
            "epoch 393 and loss is 0.005869669374078512\n",
            "epoch 394 and loss is 0.005811469629406929\n",
            "epoch 395 and loss is 0.005749962292611599\n",
            "epoch 396 and loss is 0.005703478120267391\n",
            "epoch 397 and loss is 0.0056797838769853115\n",
            "epoch 398 and loss is 0.005672035738825798\n",
            "epoch 399 and loss is 0.005666148848831654\n",
            "epoch 400 and loss is 0.0056514921598136425\n",
            "epoch 400 and loss is 0.0056514921598136425\n",
            "epoch 401 and loss is 0.005622365977615118\n",
            "epoch 402 and loss is 0.005583983846008778\n",
            "epoch 403 and loss is 0.005542846396565437\n",
            "epoch 404 and loss is 0.005507051013410091\n",
            "epoch 405 and loss is 0.005479891784489155\n",
            "epoch 406 and loss is 0.0054603745229542255\n",
            "epoch 407 and loss is 0.005444646812975407\n",
            "epoch 408 and loss is 0.00542818009853363\n",
            "epoch 409 and loss is 0.005408321041613817\n",
            "epoch 410 and loss is 0.005383690353482962\n",
            "epoch 410 and loss is 0.005383690353482962\n",
            "epoch 411 and loss is 0.0053557828068733215\n",
            "epoch 412 and loss is 0.0053260293789207935\n",
            "epoch 413 and loss is 0.005296812858432531\n",
            "epoch 414 and loss is 0.005269401706755161\n",
            "epoch 415 and loss is 0.005244523752480745\n",
            "epoch 416 and loss is 0.005221904255449772\n",
            "epoch 417 and loss is 0.005200935062021017\n",
            "epoch 418 and loss is 0.0051807826384902\n",
            "epoch 419 and loss is 0.005160787142813206\n",
            "epoch 420 and loss is 0.0051404815167188644\n",
            "epoch 420 and loss is 0.0051404815167188644\n",
            "epoch 421 and loss is 0.0051195332780480385\n",
            "epoch 422 and loss is 0.0050980509258806705\n",
            "epoch 423 and loss is 0.005075995344668627\n",
            "epoch 424 and loss is 0.0050536771304905415\n",
            "epoch 425 and loss is 0.005031078588217497\n",
            "epoch 426 and loss is 0.005008520558476448\n",
            "epoch 427 and loss is 0.004986032843589783\n",
            "epoch 428 and loss is 0.0049638026393949986\n",
            "epoch 429 and loss is 0.004941773600876331\n",
            "epoch 430 and loss is 0.004920113366097212\n",
            "epoch 430 and loss is 0.004920113366097212\n",
            "epoch 431 and loss is 0.004898791201412678\n",
            "epoch 432 and loss is 0.004877951927483082\n",
            "epoch 433 and loss is 0.00485756853595376\n",
            "epoch 434 and loss is 0.004837869666516781\n",
            "epoch 435 and loss is 0.004818941466510296\n",
            "epoch 436 and loss is 0.004801241215318441\n",
            "epoch 437 and loss is 0.0047849309630692005\n",
            "epoch 438 and loss is 0.004771112930029631\n",
            "epoch 439 and loss is 0.004760361276566982\n",
            "epoch 440 and loss is 0.00475504994392395\n",
            "epoch 440 and loss is 0.00475504994392395\n",
            "epoch 441 and loss is 0.004756465554237366\n",
            "epoch 442 and loss is 0.004770518280565739\n",
            "epoch 443 and loss is 0.004799248650670052\n",
            "epoch 444 and loss is 0.004856644663959742\n",
            "epoch 445 and loss is 0.004941381048411131\n",
            "epoch 446 and loss is 0.005082438234239817\n",
            "epoch 447 and loss is 0.005246215965598822\n",
            "epoch 448 and loss is 0.005465302150696516\n",
            "epoch 449 and loss is 0.0055711236782372\n",
            "epoch 450 and loss is 0.0055480594746768475\n",
            "epoch 450 and loss is 0.0055480594746768475\n",
            "epoch 451 and loss is 0.005164380185306072\n",
            "epoch 452 and loss is 0.004680423531681299\n",
            "epoch 453 and loss is 0.0044542234390974045\n",
            "epoch 454 and loss is 0.004637041129171848\n",
            "epoch 455 and loss is 0.004889289382845163\n",
            "epoch 456 and loss is 0.004842003807425499\n",
            "epoch 457 and loss is 0.004563514608889818\n",
            "epoch 458 and loss is 0.004368396010249853\n",
            "epoch 459 and loss is 0.004456200171262026\n",
            "epoch 460 and loss is 0.00460828747600317\n",
            "epoch 460 and loss is 0.00460828747600317\n",
            "epoch 461 and loss is 0.0045563536696136\n",
            "epoch 462 and loss is 0.00437022652477026\n",
            "epoch 463 and loss is 0.00428567361086607\n",
            "epoch 464 and loss is 0.0043659391812980175\n",
            "epoch 465 and loss is 0.004427115432918072\n",
            "epoch 466 and loss is 0.004345019347965717\n",
            "epoch 467 and loss is 0.0042303563095629215\n",
            "epoch 468 and loss is 0.004219705238938332\n",
            "epoch 469 and loss is 0.004274921957403421\n",
            "epoch 470 and loss is 0.004271319136023521\n",
            "epoch 470 and loss is 0.004271319136023521\n",
            "epoch 471 and loss is 0.004191968124359846\n",
            "epoch 472 and loss is 0.004137544427067041\n",
            "epoch 473 and loss is 0.004153115674853325\n",
            "epoch 474 and loss is 0.004172723274677992\n",
            "epoch 475 and loss is 0.004139293450862169\n",
            "epoch 476 and loss is 0.004083122126758099\n",
            "epoch 477 and loss is 0.0040639108046889305\n",
            "epoch 478 and loss is 0.004076206125319004\n",
            "epoch 479 and loss is 0.004069848917424679\n",
            "epoch 480 and loss is 0.004032569471746683\n",
            "epoch 480 and loss is 0.004032569471746683\n",
            "epoch 481 and loss is 0.0039986842311918736\n",
            "epoch 482 and loss is 0.003992083482444286\n",
            "epoch 483 and loss is 0.003993081860244274\n",
            "epoch 484 and loss is 0.003975076600909233\n",
            "epoch 485 and loss is 0.003944245167076588\n",
            "epoch 486 and loss is 0.003923640586435795\n",
            "epoch 487 and loss is 0.003917988855391741\n",
            "epoch 488 and loss is 0.003910311497747898\n",
            "epoch 489 and loss is 0.003889888059347868\n",
            "epoch 490 and loss is 0.003866100450977683\n",
            "epoch 490 and loss is 0.003866100450977683\n",
            "epoch 491 and loss is 0.003851198125630617\n",
            "epoch 492 and loss is 0.003843151731416583\n",
            "epoch 493 and loss is 0.0038311330135911703\n",
            "epoch 494 and loss is 0.0038118944503366947\n",
            "epoch 495 and loss is 0.0037928030360490084\n",
            "epoch 496 and loss is 0.0037796893157064915\n",
            "epoch 497 and loss is 0.0037694370839744806\n",
            "epoch 498 and loss is 0.003755873301997781\n",
            "epoch 499 and loss is 0.0037385104224085808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs),losses)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"epoch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "uZ6LC9YCMAWy",
        "outputId": "148d1ed0-2540-4e79-fb60-d5eb22afc7f6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_eval = model.forward(x_test)\n",
        "  loss = criterion(y_eval,y_test)\n",
        "print(f\"the loss is {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gefil8pxNDCt",
        "outputId": "c69515f8-bf90-4db7-8d04-ce3bae84c270"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the loss is 0.4384576082229614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct =0\n",
        "with torch.no_grad():\n",
        "  for i,data in enumerate(x_test):\n",
        "    y_val = model.forward(data)\n",
        "    print(f\"{i+1}.) {str(y_val)} {y_test[i]}  {y_val.argmax().item()}\")\n",
        "\n",
        "    if y_val.argmax().item() == y_test[i]:\n",
        "      correct+=1\n",
        "print(f\"accuracy is {correct/len(y_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msu2jPLxNtPq",
        "outputId": "6740dadc-388e-46b5-8ac1-d10cb5077d1f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.) tensor([-8.5894,  1.0550, 15.0591]) 2  2\n",
            "2.) tensor([-11.3995,  -2.5285,  23.2762]) 2  2\n",
            "3.) tensor([-13.0535,  -2.1378,  25.6328]) 2  2\n",
            "4.) tensor([  4.7107,  16.7978, -10.7461]) 1  1\n",
            "5.) tensor([-10.9681,  -0.5287,  20.4242]) 2  2\n",
            "6.) tensor([ 11.0652,  22.4912, -21.4391]) 1  1\n",
            "7.) tensor([-6.8915,  3.7534, 11.6367]) 2  2\n",
            "8.) tensor([  5.2742,  17.4829, -11.8447]) 1  1\n",
            "9.) tensor([-9.9178,  0.6602, 17.4394]) 2  2\n",
            "10.) tensor([-12.1878,  -2.7662,  24.8568]) 2  2\n",
            "11.) tensor([-5.3122,  5.3232,  8.6352]) 2  2\n",
            "12.) tensor([ 57.0080,  46.5485, -80.3782]) 0  0\n",
            "13.) tensor([ 51.6795,  42.1082, -72.7907]) 0  0\n",
            "14.) tensor([ 13.9329,  21.4961, -24.0116]) 1  1\n",
            "15.) tensor([ 49.4827,  42.0434, -70.5107]) 0  0\n",
            "16.) tensor([-3.0045,  8.0931,  4.1675]) 2  1\n",
            "17.) tensor([ 52.2110,  42.8798, -73.7220]) 0  0\n",
            "18.) tensor([-7.0688,  2.9400, 12.1095]) 1  2\n",
            "19.) tensor([ 55.0000,  44.8746, -77.5186]) 0  0\n",
            "20.) tensor([ 45.4450,  38.1535, -64.5415]) 0  0\n",
            "21.) tensor([ 13.0289,  22.0199, -23.2928]) 1  1\n",
            "22.) tensor([-11.8138,  -1.7185,  23.1088]) 2  2\n",
            "23.) tensor([ 50.0593,  42.2928, -71.2278]) 0  0\n",
            "24.) tensor([ 54.8960,  44.6695, -77.3076]) 0  0\n",
            "25.) tensor([ 13.7408,  23.0120, -24.4834]) 1  1\n",
            "26.) tensor([ 10.5507,  21.6145, -20.5086]) 1  1\n",
            "27.) tensor([ 3.7965, 16.1889, -9.0508]) 1  1\n",
            "28.) tensor([ 12.8791,  22.5675, -23.3757]) 1  1\n",
            "29.) tensor([ 56.5319,  46.1236, -79.6852]) 0  0\n",
            "30.) tensor([ 2.7966, 14.5428, -7.0306]) 1  1\n",
            "accuracy is 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_iris = torch.tensor([5.9,3.0,5.1,1.8])"
      ],
      "metadata": {
        "id": "Ho6P7w-ZOBPE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  print(model.forward(new_iris))\n",
        "  print(model.forward(new_iris).argmax().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MgY2cNMPb9S",
        "outputId": "94c64b8d-e938-4436-8814-bce2f41e94c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-8.4194,  1.3700, 14.7262])\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),'iris_test.pt')"
      ],
      "metadata": {
        "id": "pI-CyGy2PeSH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_iris = Model()\n",
        "new_iris.load_state_dict(torch.load('iris_test.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKElxBNx4HIT",
        "outputId": "1dcb5c8e-ca4b-40ae-a270-7f5ce671233d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-0a82628b324c>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  new_iris.load_state_dict(torch.load('iris_test.pt'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_iris.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAXWrpf34VAp",
        "outputId": "62d00e34-1ee6-406a-9a5f-6762d9e31861"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=4, out_features=8, bias=True)\n",
              "  (fc2): Linear(in_features=8, out_features=9, bias=True)\n",
              "  (out): Linear(in_features=9, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJO8FhXa4dCS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}